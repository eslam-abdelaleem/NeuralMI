{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Analyzing Spike Train Data\n",
    "\n",
    "This tutorial demonstrates the end-to-end workflow for analyzing relationships between populations of spiking neurons, including a method for improving the stability of the results.\n",
    "\n",
    "**Goal:**\n",
    "1.  Demonstrate the use of the `processor_type='spike'`.\n",
    "2.  Use a sweep to find the characteristic timescale of a relationship.\n",
    "3.  Demonstrate how to perform multiple runs to get a more robust, averaged MI estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neural_mi as nmi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic Spike Data\n",
    "\n",
    "We'll simulate two neural populations, X and Y, where Y is driven by X with a ~20ms delay. Our goal is to recover this timescale from the data. The data should be a **list of numpy arrays**, where each array contains the spike times for one neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spike_data, y_spike_data = nmi.datasets.generate_correlated_spike_trains(duration=100, firing_rate=30)\\n",
    "\n",
    "# Visualize the first two neurons to see the relationship\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.eventplot(x_spike_data[0], color='C0', linelengths=0.8, lineoffsets=1, label=r'Neuron $X_1$')\n",
    "plt.eventplot(y_spike_data[0], color='C1', linelengths=0.8, lineoffsets=-1, label=r'Neuron $Y_1$')\n",
    "plt.yticks([]); \n",
    "plt.xlabel(\"Time (s)\"); \n",
    "plt.title(\"Spike Raster for First Neuron Pair\"); \n",
    "plt.xlim(0, 10); \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving Robustness with Multiple Runs\n",
    "\n",
    "Due to the randomness in neural network initialization and training, a single MI estimate can be noisy. To get a more stable result, we can run the estimation multiple times for each `window_size` and average the results.\n",
    "\n",
    "We achieve this by adding a `run_id` parameter to our sweep grid. By providing `range(5)`, we tell the sweep engine to perform 5 independent runs for every other parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_params = {\n",
    "    'n_epochs': 100, 'learning_rate': 1e-3, 'batch_size': 128,\n",
    "    'patience': 10, 'embedding_dim': 16, 'hidden_dim': 64, 'n_layers': 2\n",
    "}\n",
    "\n",
    "sweep_grid = {\n",
    "    'window_size': [0.005, 0.01, 0.02, 0.03, 0.05, 0.08, 0.1, 0.15],\n",
    "    'run_id': range(5) # Perform 5 runs for each window size\n",
    "}\n",
    "\n",
    "# Pass the raw spike data (list of arrays) directly to the function\n",
    "sweep_results = nmi.run(\n",
    "    x_data=x_spike_data,\n",
    "    y_data=y_spike_data,\n",
    "    mode='sweep',\n",
    "    base_params=base_params,\n",
    "    sweep_grid=sweep_grid,\n",
    "    processor_type='spike',\n",
    "    processor_params={'step_size': 0.001},\n",
    "    n_workers=4\n",
    ")\n",
    "\n",
    "display(sweep_results.dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing the Averaged Results\n",
    "\n",
    "The output `Results` object contains a DataFrame with multiple `test_mi` values for each `window_size`. We can use `pandas` to group by `window_size` and calculate the mean and standard deviation, giving us a much more reliable picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the swept parameter and aggregate\n",
    "summary_df = sweep_results.dataframe.groupby('window_size')['test_mi'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Find the optimal window size that maximizes the mean MI\n",
    "best_window_size = summary_df.loc[summary_df['mean'].idxmax()]['window_size']\n",
    "print(f\"Optimal Window Size: {best_window_size*1000:.1f} ms\")\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Use the generic plotting function on our aggregated data\n",
    "nmi.visualize.plot_sweep_curve(\n",
    "    summary_df=summary_df,\n",
    "    param_col='window_size',\n",
    "    mean_col='mean',\n",
    "    std_col='std',\n",
    "    estimated_values={'Optimal': best_window_size},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.axvline(x=0.02, color='green', linestyle='--', label='True Delay (20ms)')\n",
    "ax.set_xlabel(\"Window Size (seconds)\")\n",
    "ax.set_title(\"MI vs. Window Size for Spike Data (Averaged)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}