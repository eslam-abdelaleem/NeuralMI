{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: The Importance of Time\n",
    "\n",
    "In the first example, we assumed each sample $(x_i, y_i)$ was independent. However, in most biological and neural data, relationships are spread out over time. A stimulus now might affect a neural response hundreds of milliseconds later.\n",
    "\n",
    "This notebook demonstrates how to handle such temporal dependencies.\n",
    "\n",
    "**Goal:**\n",
    "1.  Introduce the `data.processors` for windowing continuous data.\n",
    "2.  Showcase `run(mode='sweep')` to find the optimal `window_size`.\n",
    "3.  Demonstrate why choosing the correct timescale is critical for MI estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We'll need our standard imports, plus `matplotlib` for plotting the results of our sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neural_mi as nmi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use a nice style for plotting\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Temporally Dependent Data\n",
    "\n",
    "We'll use `generate_temporally_convolved_data`. This function creates a latent signal `Z`, and then generates `X` and `Y` by smearing `Z` with different time kernels. This means the relationship between a single point $x_t$ and $y_t$ is weak, but the relationship between a *window* of X and a *window* of Y is strong.\n",
    "\n",
    "For this type of data, the ground truth MI isn't easily known, which is realistic. Our goal is not to match a known value, but to find the timescale that *maximizes* the MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Parameters ---\n",
    "n_samples = 10000\n",
    "\n",
    "# --- Generate Data ---\n",
    "# The output shape is [1, n_samples] which represents [n_channels, n_timepoints]\n",
    "x_raw, y_raw = nmi.datasets.generate_temporally_convolved_data(n_samples=n_samples, use_torch=False)\n",
    "\n",
    "print(f\"Generated raw X data shape: {x_raw.shape}\")\n",
    "print(f\"Generated raw Y data shape: {y_raw.shape}\")\n",
    "\n",
    "# Let's visualize the raw data to see the temporal relationship\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_raw[0, :200], label='X', alpha=0.8)\n",
    "plt.plot(y_raw[0, :200], label='Y', alpha=0.8)\n",
    "plt.xlabel(\"Timepoints\")\n",
    "plt.ylabel(\"Signal\")\n",
    "plt.title(\"Raw Temporal Data (first 200 points)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Problem: A Naive Estimate Fails\n",
    "\n",
    "If we treat each timepoint as an independent sample (a window size of 1), we fail to capture the smeared relationship. Let's prove this by processing the data with a `window_size=1` and running a quick estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data with a window size of 1\n",
    "processor = nmi.data.ContinuousProcessor(window_size=1, step_size=1)\n",
    "x_naive = processor.process(x_raw)\n",
    "y_naive = processor.process(y_raw)\n",
    "\n",
    "print(f\"Shape after processing with window_size=1: {x_naive.shape}\")\n",
    "\n",
    "# Use the same base parameters as before\n",
    "base_params = {\n",
    "    'n_epochs': 50, 'learning_rate': 1e-3, 'batch_size': 128,\n",
    "    'patience': 5, 'embedding_dim': 16, 'hidden_dim': 64, 'n_layers': 2\n",
    "}\n",
    "\n",
    "naive_mi = nmi.run(x_data=x_naive, y_data=y_naive, mode='estimate', base_params=base_params)\n",
    "print(f\"\\nNaive MI estimate (window_size=1): {naive_mi:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the MI estimate is very low. We've missed the real relationship.\n",
    "\n",
    "## 4. The Solution: Sweeping Over Window Size\n",
    "\n",
    "To find the correct timescale, we need to test many different window sizes. This is a hyperparameter search, which is exactly what `mode='sweep'` is for.\n",
    "\n",
    "The process is:\n",
    "1.  Define a `sweep_grid`. This dictionary tells the `run` function which parameters to vary. Our key will be `window_size`.\n",
    "2.  The `run` function will iterate through each value in the grid.\n",
    "3.  **Crucially, we do not pre-process the data.** The sweep engine handles the processing internally for each `window_size`.\n",
    "\n",
    "*Note: The parameter `window_size` is not a model parameter, it's a data processing parameter. The library is designed to handle this by looking for specific keys in the sweep grid and applying them to the processor before training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sweep will create a new ContinuousProcessor for each value.\n",
    "sweep_grid = {\n",
    "    'window_size': [1, 5, 10, 15, 20, 25, 30, 40, 50, 100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Notice we pass the RAW data to the run function\n",
    "sweep_results_df = nmi.run(\n",
    "    x_data=torch.from_numpy(x_raw).float(), # The runner needs tensors\n",
    "    y_data=torch.from_numpy(y_raw).float(),\n",
    "    mode='sweep',\n",
    "    base_params=base_params,\n",
    "    sweep_grid=sweep_grid,\n",
    "    # This tells the run function to perform a processing sweep.\n",
    "    processor_type='continuous',\n",
    "    # Speed up the sweep with parallel workers\n",
    "    n_workers=4\n",
    ")\n",
    "\n",
    "display(sweep_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Results\n",
    "\n",
    "The output is a pandas DataFrame containing the results for each hyperparameter combination. Now we can simply plot the `test_mi` against the `window_size` to find the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = sweep_results_df.loc[sweep_results_df['test_mi'].idxmax()]\n",
    "\n",
    "# --- Now create the plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=sweep_results_df, x='window_size', y='test_mi', marker='o')\n",
    "plt.axvline(x=best_run['window_size'], c='r', ls=':', label=f\"Optimal ({best_run['window_size']})\")\n",
    "plt.xlabel(\"Window Size (timepoints)\")\n",
    "plt.ylabel(\"Estimated MI (bits)\")\n",
    "plt.title(\"MI vs. Window Size\")\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"--- Best Result ---\")\n",
    "print(f\"Optimal Window Size: {best_run['window_size']}\")\n",
    "print(f\"Maximum MI Estimated: {best_run['test_mi']:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "The result is clear! The estimated MI peaks at a window size around $100\\sim 200$ timepoints. This is few times the characteristic timescale of the relationship in our generated data. By using the right window, we recovered a strong MI ($\\sim 5$ bits) that was completely invisible to the naive, point-by-point estimate ($\\sim 2$ bits) --numbers can differ slightly--.\n",
    "\n",
    "This example demonstrates a core workflow for analyzing real experimental data:\n",
    "1.  Start with raw time-series data.\n",
    "2.  Use a `sweep` over `window_size` to find the timescale that maximizes information.\n",
    "3.  This optimal window size is itself a valuable scientific finding.\n",
    "\n",
    "In the next example, we'll explore the internal structure of a single, high-dimensional dataset to estimate its 'latent dimensionality'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
