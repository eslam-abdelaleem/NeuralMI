{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: The Importance of Time\n",
    "\n",
    "In the first example, we assumed each sample $(x_i, y_i)$ was independent. However, in most biological and neural data, relationships are spread out over time. A stimulus now might affect a neural response hundreds of milliseconds later.\n",
    "\n",
    "This notebook demonstrates how to handle such temporal dependencies.\n",
    "\n",
    "**Goal:**\n",
    "1.  Demonstrate how to handle raw time-series data using the built-in processor.\n",
    "2.  Showcase `run(mode='sweep')` to find the optimal `window_size`.\n",
    "3.  Demonstrate why choosing the correct timescale is critical for MI estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We'll need our standard imports, plus `matplotlib` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neural_mi as nmi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Temporally Dependent Data\n",
    "\n",
    "We'll use `generate_temporally_convolved_data`. This function creates a latent signal `Z`, and then generates `X` and `Y` by smearing `Z` with different time kernels. This means the relationship between a single point $x_t$ and $y_t$ is weak, but the relationship between a *window* of X and a *window* of Y is strong.\n",
    "\n",
    "For this type of data, the ground truth MI isn't easily known, which is realistic. Our goal is not to match a known value, but to find the timescale that *maximizes* the MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Parameters ---\n",
    "n_samples = 10000\n",
    "\n",
    "# --- Generate Raw 2D Data ---\n",
    "# The output shape is [n_timepoints, n_channels]\n",
    "x_raw, y_raw = nmi.datasets.generate_temporally_convolved_data(n_samples=n_samples)\n",
    "\n",
    "print(f\"Generated raw X data shape: {x_raw.shape}\")\n",
    "print(f\"Generated raw Y data shape: {y_raw.shape}\")\n",
    "\n",
    "# Let's visualize the raw data to see the temporal relationship\n",
    "plt.figure(figsize=(12, 4))\n",
    "# The data is shape [n_channels, n_timepoints], so we select the first channel\n",
    "plt.plot(x_raw[0, :200], label='X', alpha=0.8)\n",
    "plt.plot(y_raw[0, :200], label='Y', alpha=0.8)\n",
    "plt.xlabel(\"Timepoints\")\n",
    "plt.ylabel(\"Signal\")\n",
    "plt.title(\"Raw Temporal Data (first 200 points)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Problem: A Naive Estimate Fails\n",
    "\n",
    "If we treat each timepoint as an independent sample (a window size of 1), we fail to capture the smeared relationship. Let's prove this by running a quick estimate with `window_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {\n",
    "    'n_epochs': 50, 'learning_rate': 1e-3, 'batch_size': 128,\n",
    "    'patience': 5, 'embedding_dim': 16, 'hidden_dim': 64, 'n_layers': 2\n",
    "}\n",
    "\n",
    "# Run the estimate with a window size of 1\n",
    "naive_results = nmi.run(\n",
    "    x_data=x_raw,\n",
    "    y_data=y_raw,\n",
    "    mode='estimate',\n",
    "    processor_type='continuous',\n",
    "    processor_params={'window_size': 1},\n",
    "    base_params=base_params\n",
    ")\n",
    "\n",
    "print(f\"\\nNaive MI estimate (window_size=1): {naive_results.mi_estimate:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the MI estimate is very low. We've missed the real relationship.\n",
    "\n",
    "## 4. The Solution: Sweeping Over Window Size\n",
    "\n",
    "To find the correct timescale, we need to test many different window sizes. This is a hyperparameter search, which is exactly what `mode='sweep'` is for.\n",
    "\n",
    "The process is simple:\n",
    "1.  Define a `sweep_grid`. This dictionary tells the `run` function which parameters to vary. Our key will be `window_size`.\n",
    "2.  Pass the **raw data** and the `sweep_grid` to `nmi.run`.\n",
    "3.  The library automatically applies the processor with each `window_size` from the grid before training the MI estimator.\n",
    "\n",
    "*Note: The parameter `window_size` is a data processing parameter, not a model parameter. The library is designed to handle this by automatically routing processing-related keys from the `sweep_grid` to the `DataHandler`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sweep will create a new ContinuousProcessor for each value.\n",
    "sweep_grid = {\n",
    "    'window_size': [1, 5, 10, 15, 20, 25, 30, 40, 50, 100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Notice we pass the RAW data to the run function\n",
    "sweep_results = nmi.run(\n",
    "    x_data=x_raw,\n",
    "    y_data=y_raw,\n",
    "    mode='sweep',\n",
    "    processor_type='continuous',\n",
    "    processor_params={},\n",
    "    base_params=base_params,\n",
    "    sweep_grid=sweep_grid,\n",
    "    n_workers=4  # Speed up the sweep with parallel workers\n",
    ")\n",
    "\n",
    "display(sweep_results.dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Results\n",
    "\n",
    "The output is a `Results` object containing a DataFrame. We can now either plot the MI curve manually, or use the built-in `.plot()` method for a quick visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built-in plot function!\n",
    "sweep_results.plot()\n",
    "plt.title(\"MI vs. Window Size\")\n",
    "plt.xscale('log')\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()\n",
    "\n",
    "# Find the best run programmatically\n",
    "best_run = sweep_results.dataframe.loc[sweep_results.dataframe['mi_mean'].idxmax()]\n",
    "print(\"--- Best Result ---\")\n",
    "print(f\"Optimal Window Size: {best_run['window_size']}\")\n",
    "print(f\"Maximum MI Estimated: {best_run['mi_mean']:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "The result is clear! The estimated MI peaks at a window size around $100\\sim 200$ timepoints. By using the right window, we recovered a strong MI that was completely invisible to the naive, point-by-point estimate.\n",
    "\n",
    "This example demonstrates a core workflow for analyzing real experimental data:\n",
    "1.  Start with raw time-series data.\n",
    "2.  Use a `sweep` over `window_size` to find the timescale that maximizes information.\n",
    "3.  This optimal window size is itself a valuable scientific finding.\n",
    "\n",
    "In the next example, we'll explore the internal structure of a single, high-dimensional dataset to estimate its 'latent dimensionality'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}