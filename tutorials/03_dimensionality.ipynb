{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Finding Hidden Signals (Latent Dimensionality)\n",
    "\n",
    "So far, we've focused on the relationship *between* two variables, X and Y. But what if we want to understand the internal complexity of a *single* high-dimensional dataset? For example, how many independent signals are present in a recording from 100 neurons?\n",
    "\n",
    "This is the problem of estimating **latent dimensionality**. This notebook demonstrates how to use `NeuralMI` to do just that.\n",
    "\n",
    "**Goal:**\n",
    "1.  Introduce the concept of \"Internal Information\" ($I(X_A; X_B)$).\n",
    "2.  Use `run(mode='dimensionality')` to automate this analysis.\n",
    "3.  Generate data with a known `latent_dim` and see if we can recover it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neural_mi as nmi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Concept: Internal Information\n",
    "\n",
    "To measure the internal complexity of a dataset `X` (e.g., shape `[n_samples, n_neurons, n_features]`), we can't just compute $I(X;X)$, as that would be infinite. Instead, we do the following:\n",
    "\n",
    "1.  Randomly split the channels (neurons) of `X` into two non-overlapping halves, $X_A$ and $X_B$.\n",
    "2.  Calculate the mutual information between these halves: $I(X_A; X_B)$.\n",
    "3.  Repeat this process for many different random splits and average the results for robustness.\n",
    "\n",
    "This **Internal Information** tells us how much redundancy or shared information exists within the channels of `X`. If the neurons are all independent, this value will be zero. If they are highly coordinated (driven by a shared latent signal), this value will be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Data with a Known Latent Structure\n",
    "\n",
    "We'll use `generate_nonlinear_from_latent`. This function is perfect for our goal. It will:\n",
    "1.  Create a simple, low-dimensional latent signal (e.g., 4-dimensional).\n",
    "2.  Use a nonlinear neural network to \"project\" this signal up to a high-dimensional observation (e.g., 50 dimensions, representing 50 neurons).\n",
    "\n",
    "Our goal is to see if the library can analyze the 50-dimensional data and correctly deduce that the underlying, hidden dimensionality is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Parameters ---\n",
    "n_samples = 10000\n",
    "true_latent_dim = 4      # The ground truth we want to recover\n",
    "observed_dim = 50        # The number of \"neurons\" we observe\n",
    "mi_between_latents = 3.0 # Strength of correlation in the latent space\n",
    "\n",
    "# --- Generate Raw 2D Data ---\n",
    "# We only need a single variable 'x_data' for this analysis.\n",
    "# Shape: [n_samples, observed_dim]\n",
    "x_raw, _ = nmi.datasets.generate_nonlinear_from_latent(\n",
    "    n_samples=n_samples, \n",
    "    latent_dim=true_latent_dim,\n",
    "    observed_dim=observed_dim,\n",
    "    mi=mi_between_latents\n",
    ")\n",
    "\n",
    "print(f\"Generated raw X data shape: {x_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Dimensionality Analysis\n",
    "\n",
    "The key idea is to see how the Internal Information changes as we vary the capacity of our MI estimation model. We do this by sweeping over the `embedding_dim` parameter.\n",
    "\n",
    "We expect the MI to increase as `embedding_dim` increases, but it should **plateau or saturate** once `embedding_dim` is large enough to capture the true latent dimensionality of the data. The location of this \"elbow\" is our estimate.\n",
    "\n",
    "`run(mode='dimensionality')` automates this entire process. We pass the raw data and tell the processor to treat each row as an independent sample (`window_size=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base parameters for the trainer\n",
    "base_params = {\n",
    "    'n_epochs': 50, 'learning_rate': 1e-3, 'batch_size': 128,\n",
    "    'patience': 5, 'hidden_dim': 128, 'n_layers': 3\n",
    "}\n",
    "\n",
    "# The sweep_grid MUST contain 'embedding_dim' for this mode\n",
    "sweep_grid = {\n",
    "    'embedding_dim': [1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 20]\n",
    "}\n",
    "\n",
    "# Run the analysis directly on the raw, 2D data\n",
    "dim_results = nmi.run(\n",
    "    x_data=x_raw,\n",
    "    # y_data is not needed for this mode\n",
    "    mode='dimensionality',\n",
    "    processor_type='continuous',\n",
    "    processor_params={'window_size': 1},\n",
    "    base_params=base_params,\n",
    "    sweep_grid=sweep_grid,\n",
    "    # n_splits controls how many random channel splits to average over\n",
    "    n_splits=5,\n",
    "    n_workers=4\n",
    ")\n",
    "\n",
    "display(dim_results.dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing and Interpreting the Saturation Curve\n",
    "\n",
    "The output is a `Results` object containing a DataFrame. We can use the built-in `.plot()` method to see the curve, and the `find_saturation_point` helper to estimate the elbow point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the `strictness` Parameter\n",
    "\n",
    "The `find_saturation_point` function has a `strictness` parameter that naively controls how flat the curve must be to be considered a plateau. It's basically checks if  `MI(k) - MI(k-1) < strictness * STD(k)`.\n",
    "- **Low `strictness`** (e.g., <= 1) is 'stricter'. It requires a very flat plateau, so it might estimate a higher dimensionality if the curve is slow to level off.\n",
    "- **High `strictness`** (e.g., >= 1) is 'looser'. It will declare saturation earlier, even if the curve is still rising slightly.\n",
    "\n",
    "By default, the function now tests a range of strictness values to give you a more complete picture of the potential estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically find the saturation point for a range of strictness values\n",
    "estimated_dims = nmi.utils.find_saturation_point(dim_results.dataframe, strictness=[0.1, 1, 15])\n",
    "print(f\"True Latent Dimension: {true_latent_dim}\")\n",
    "print(f\"Estimated Latent Dimensions: {estimated_dims}\")\n",
    "\n",
    "# Plot the curve using the built-in plot method\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "dim_results.plot(ax=ax, show=False) # Pass the axes to the plot function\n",
    "\n",
    "# Add annotations for the true and estimated values\n",
    "ax.axvline(x=true_latent_dim, color='black', linestyle='--', label=f'True Dim ({true_latent_dim})')\n",
    "for s, est_dim in estimated_dims.items():\n",
    "    ax.axvline(x=est_dim, linestyle=':', label=f'Est. Dim (strict={s})')\n",
    "\n",
    "ax.set_title('Internal Information vs. Embedding Dimension')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "The result is fantastic! The plot clearly shows the MI estimate rising steadily and then flattening out right around the true latent dimension of 4. The automated `find_saturation_point` function successfully identifies this elbow.\n",
    "\n",
    "This demonstrates a powerful exploratory capability of the `NeuralMI` library. You can take a high-dimensional neural recording, run this analysis, and get a quantitative estimate of its intrinsic complexity or the number of independent signals it contains.\n",
    "\n",
    "In the next tutorial, we will tackle the most advanced feature: performing a rigorous, bias-corrected analysis to get accurate MI estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}