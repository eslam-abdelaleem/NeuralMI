{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: From Naive Estimates to Rigorous Analysis\n",
    "\n",
    "Using any MI estimator naively on a neural system is rarely a good idea. While our `'estimate'` mode is sophisticated, it doesn't account for a critical statistical pitfall: **finite-sampling bias**. With limited data, estimators tend to find spurious correlations, leading to an overestimation of the true MI. This means that if you tried to calcauate MI when you have a certain number of samples, then tried again using a different subset of those samples, you'll get a different answer --unless you have infinite amount of data--.\n",
    "\n",
    "This final notebook demonstrates how to move from a simple estimate to a rigorous one by correcting for this bias and producing principled error bars.\n",
    "\n",
    "**Goal:**\n",
    "1.  Showcase the bias problem on a complex, nonlinear dataset.\n",
    "2.  Use `run(mode='rigorous')` to get a debiased estimate and a confidence interval.\n",
    "3.  Clarify the `[samples, channels, features]` data shape convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neural_mi as nmi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Complex, Biologically-Plausible Data\n",
    "\n",
    "To highlight the need for rigorous analysis, we'll use our most complex dataset, `generate_nonlinear_from_latent`. This mimics a scenario where two high-dimensional neural populations (`X` and `Y`) are driven by a shared, low-dimensional latent signal. \n",
    "\n",
    "We'll use a small number of samples (`n_samples=1500`) to ensure that finite-sampling bias will be a significant problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "latent_dim = 4\n",
    "observed_dim = 100 # Each variable X and Y has 20 dimensions\n",
    "latent_mi_bits = 3.0 # The MI between the *hidden* variables\n",
    "\n",
    "x_data, y_data = nmi.datasets.generate_nonlinear_from_latent(\n",
    "    n_samples=n_samples, \n",
    "    latent_dim=latent_dim,\n",
    "    observed_dim=observed_dim,\n",
    "    mi=latent_mi_bits\n",
    ")\n",
    "\n",
    "print(f\"Raw X data shape: {x_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Data Shape: `[samples, channels, features]`\n",
    "\n",
    "For an $I(X;Y)$ analysis, we treat each variable as a single entity. The dimensions *within* it are its **features**. Since `X` is a single entity with `observed_dim` features, we reshape it to have **1 channel**. This is the standard convention for this type of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.reshape(n_samples, 1, observed_dim)\n",
    "y_data = y_data.reshape(n_samples, 1, observed_dim)\n",
    "\n",
    "print(f\"Reshaped X data shape: {x_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Problem: A Naive Estimate is Unreliable\n",
    "\n",
    "Let's run a simple `'estimate'` on this data. While we don't know the exact ground truth MI in the observed space (due to the nonlinear transform), we will see that the rigorous estimate provides a very different, more reliable answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {\n",
    "    'n_epochs': 60, 'learning_rate': 1e-3, 'batch_size': 128,\n",
    "    'patience': 7, 'embedding_dim': 20, 'hidden_dim': 128, 'n_layers': 3\n",
    "}\n",
    "\n",
    "naive_mi = nmi.run(x_data=x_data, y_data=y_data, mode='estimate', base_params=base_params)\n",
    "\n",
    "print(\"--- Naive Estimate ---\")\n",
    "print(f\"Naive Estimated MI:   {naive_mi:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it again, notice the first problem: each run gives slightly different answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_mi = nmi.run(x_data=x_data, y_data=y_data, mode='estimate', base_params=base_params)\n",
    "\n",
    "print(\"--- Naive Estimate ---\")\n",
    "print(f\"Naive Estimated MI #2:   {naive_mi:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it again but with half the data, notice the second problem: finite sizes make difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_mi = nmi.run(x_data=x_data[:n_samples//2], y_data=y_data[:n_samples//2], mode='estimate', base_params=base_params)\n",
    "\n",
    "print(\"--- Naive Estimate ---\")\n",
    "print(f\"Naive Estimated MI For half the data:   {naive_mi:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Solution: Rigorous, Bias-Corrected Estimation\n",
    "\n",
    "MI is a property of the distributions, and it shouldn't depend on the samples. However, since we don't have access to the distrubtions, and unless we have infinite amount of data, then we will have finite sample size effects, and we need to correct for them.\n",
    "\n",
    "The `'rigorous'` mode performs subsampling and extrapolation to remove the finite-sampling bias. This provides a more accurate point estimate and, just as importantly, a principled confidence interval (error bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rigorous_output = nmi.run(\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    mode='rigorous',\n",
    "    base_params=base_params,\n",
    "    sweep_grid={'embedding_dim': [16]},\n",
    "    n_workers=4\n",
    ")\n",
    "\n",
    "corrected_results = rigorous_output['corrected_results'][0]\n",
    "raw_results_df = rigorous_output['raw_results_df']\n",
    "\n",
    "corrected_mi = corrected_results['mi_corrected']\n",
    "mi_error = corrected_results['mi_error']\n",
    "\n",
    "print(\"\\n--- Rigorous Estimate ---\")\n",
    "print(f\"Naive Estimate:   {naive_mi:.3f} bits\")\n",
    "print(f\"Corrected MI:     {corrected_mi:.3f} Â± {mi_error:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Correction\n",
    "\n",
    "The plot shows the raw MI estimates for each number of subsets ($\\gamma$) and the extrapolation line that leads to the final, debiased estimate. Notice how different runs/fractions give different results, and how more trustworthy extrapolated result are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "nmi.visualize.plot_bias_correction_fit(\n",
    "    raw_results_df=raw_results_df,\n",
    "    corrected_result=corrected_results,\n",
    "    ax=ax\n",
    ")\n",
    "ax.axhline(y=naive_mi, color='purple', linestyle='--', label=f'Naive Estimate ({naive_mi:.2f} bits)')\n",
    "ax.axhline(y=latent_mi_bits, color='green', linestyle='-', label=f'True MI ({latent_mi_bits:.2f} bits)')\n",
    "ax.legend()\n",
    "ax.set_ylim(-0.1, 3.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
