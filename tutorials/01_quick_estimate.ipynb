{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: A Quick First Estimate\n",
    "\n",
    "This notebook covers the most basic use case of the `NeuralMI` library: getting a single, quick estimate of mutual information between two variables.\n",
    "\n",
    "**Goal:**\n",
    "1.  Introduce the main `neural_mi.run` function.\n",
    "2.  Use a simple dataset where the ground truth MI is known analytically.\n",
    "3.  Compare our estimate to the ground truth to verify the library is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We'll need `torch` for data handling, our `run` function, and the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neural_mi as nmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating the Data\n",
    "\n",
    "We will use the `generate_correlated_gaussians` function from the `datasets` module. This function creates two multidimensional Gaussian variables, `X` and `Y`, where we can precisely specify the mutual information between them in **bits**.\n",
    "\n",
    "The analytical formula for MI between two multivariate Gaussians is:\n",
    "\n",
    "$$ I(X;Y) = -\\frac{1}{2} \\log_2 \\det(\\Sigma_{XY}) $$\n",
    "\n",
    "Where $\\Sigma_{XY}$ is the correlation matrix. Our generator function handles this for us. Let's create data with a known MI of **2.0 bits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Parameters ---\n",
    "n_samples = 5000\n",
    "dim = 5\n",
    "ground_truth_mi_bits = 2.0\n",
    "\n",
    "# --- Generate Raw 2D Data ---\n",
    "# This creates data of shape [n_samples, dim].\n",
    "x_raw, y_raw = nmi.datasets.generate_correlated_gaussians(\n",
    "    n_samples=n_samples, \n",
    "    dim=dim, \n",
    "    mi=ground_truth_mi_bits\n",
    ")\n",
    "\n",
    "print(f\"Generated raw X data shape: {x_raw.shape}\")\n",
    "print(f\"Generated raw Y data shape: {y_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Analysis Parameters\n",
    "\n",
    "The `run` function requires a `base_params` dictionary. This tells the internal `Trainer` how to configure the neural network and the training process. \n",
    "\n",
    "Since we are passing raw data, we also need to tell the library how to process it. We'll specify `processor_type='continuous'` and provide the `processor_params`. In this simple case, each sample is independent, so we use a `window_size` of 1, which tells the processor to treat each row as a distinct sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processor will treat each row as a sample.\n",
    "processor_params = {'window_size': 1}\n",
    "\n",
    "# Basic model and training parameters\n",
    "base_params = {\n",
    "    'n_epochs': 50,          # Number of training epochs\n",
    "    'learning_rate': 1e-3,   # Learning rate for the optimizer\n",
    "    'batch_size': 128,       # Batch size for training\n",
    "    'patience': 5,           # Early stopping patience\n",
    "    \n",
    "    # --- Network Architecture ---\n",
    "    'embedding_dim': 16,     # Dimensionality of the learned embeddings\n",
    "    'hidden_dim': 64,        # Number of units in hidden layers\n",
    "    'n_layers': 2            # Number of hidden layers in the MLP\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the MI Estimation\n",
    "\n",
    "Now we call the main `nmi.run` function. We provide our raw data and the processing parameters. The library handles the rest.\n",
    "\n",
    "The function returns a standardized `Results` object, which contains the MI estimate and other useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nmi.run(\n",
    "    x_data=x_raw,             # Pass raw 2D data\n",
    "    y_data=y_raw,             # Pass raw 2D data\n",
    "    mode='estimate',\n",
    "    processor_type='continuous', # Specify the processor\n",
    "    processor_params=processor_params,\n",
    "    base_params=base_params,\n",
    "    output_units='bits'\n",
    ")\n",
    "\n",
    "# Access the estimate from the Results object\n",
    "estimated_mi_bits = results.mi_estimate\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"Ground Truth MI:      {ground_truth_mi_bits:.3f} bits\")\n",
    "print(f\"Estimated MI:         {estimated_mi_bits:.3f} bits\")\n",
    "print(f\"Estimation Error:     {abs(estimated_mi_bits - ground_truth_mi_bits):.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Success! The estimated MI is very close to the ground truth value we specified. We were able to get this estimate without manually reshaping or processing our data, because the `DataHandler` inside the `run` function took care of it for us.\n",
    "\n",
    "In the next example, we'll tackle a more complex problem where the relationship between X and Y isn't instantaneous, demonstrating the power of the built-in windowing processor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}