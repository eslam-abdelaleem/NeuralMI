

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Theoretical Foundations of NeuralMI &mdash; NeuralMI 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" />

  
    <link rel="canonical" href="/NeuralMI/THEORY.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8d563738"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Under the Hood: How NeuralMI Works" href="UNDER_THE_HOOD.html" />
    <link rel="prev" title="Further Reading" href="further_reading.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            NeuralMI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="further_reading.html">Further Reading</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Theoretical Foundations of NeuralMI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-challenge-of-estimating-mutual-information">1. The Challenge of Estimating Mutual Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-mi-estimators-a-bias-variance-trade-off">2. Neural MI Estimators: A Bias-Variance Trade-off</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#infonce-low-variance-high-bias">2.1 InfoNCE (Low Variance, High Bias)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#smile-low-bias-moderate-variance">2.2 SMILE (Low Bias, Moderate Variance)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-variational-approach">3. The Variational Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-problem-of-finite-sampling-bias">4. The Problem of Finite-Sampling Bias</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-solution-rigorous-bias-correction">5. The Solution: Rigorous Bias Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#estimating-latent-dimensionality">6. Estimating Latent Dimensionality</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="UNDER_THE_HOOD.html">Under the Hood: How NeuralMI Works</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NeuralMI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="further_reading.html">Further Reading</a></li>
      <li class="breadcrumb-item active">Theoretical Foundations of NeuralMI</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/THEORY.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="theoretical-foundations-of-neuralmi">
<h1>Theoretical Foundations of NeuralMI<a class="headerlink" href="#theoretical-foundations-of-neuralmi" title="Link to this heading"></a></h1>
<p>This document provides a concise theoretical background for the core methods used in the <code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code> library. It is intended as a formal reference for users who wish to understand the mathematical principles behind the code.</p>
<section id="the-challenge-of-estimating-mutual-information">
<h2>1. The Challenge of Estimating Mutual Information<a class="headerlink" href="#the-challenge-of-estimating-mutual-information" title="Link to this heading"></a></h2>
<p>Mutual Information (MI) is formally defined as the Kullback-Leibler (KL) divergence between the joint distribution $p(x, y)$ and the product of the marginal distributions $p(x)p(y)$:</p>
<div class="math notranslate nohighlight">
\[I(X; Y) = \int p(x, y) \log \frac{p(x, y)}{p(x)p(y)} dx dy\]</div>
<p>Calculating this directly requires knowing these probability distributions. For high-dimensional and continuous data, like that often found in neuroscience, these distributions are unknown and practically impossible to estimate accurately. Traditional methods like binning or kernel density estimation fail due to the “curse of dimensionality.”</p>
<p>To overcome this, <code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code> uses a modern approach called <strong>neural estimation</strong>, which reframes MI estimation as a neural network optimization problem.</p>
</section>
<section id="neural-mi-estimators-a-bias-variance-trade-off">
<h2>2. Neural MI Estimators: A Bias-Variance Trade-off<a class="headerlink" href="#neural-mi-estimators-a-bias-variance-trade-off" title="Link to this heading"></a></h2>
<p>Instead of estimating the probability densities, we can use a neural network, called a <strong>critic</strong> $f(x, y)$, to help us estimate a lower bound on the true MI. The core idea is to train the critic to distinguish between “positive” samples (pairs <code class="docutils literal notranslate"><span class="pre">(x_i,</span> <span class="pre">y_i)</span></code> that genuinely occurred together) and “negative” samples (pairs <code class="docutils literal notranslate"><span class="pre">(x_i,</span> <span class="pre">y_j)</span></code> from the same batch that did not).</p>
<p>Different mathematical formulations, or estimators, can be used for this task. They represent a trade-off between the <strong>bias</strong> of the estimate (how far it is from the true value on average) and its <strong>variance</strong> (how much it fluctuates on different runs). As argued in recent literature, choosing the right estimator depends on the scientific question.</p>
<blockquote>
<div><p><strong>References:</strong></p>
<ul class="simple">
<li><p>“Understanding the Limitations of Variational Mutual Information Estimators” (ICLR 2020)</p></li>
<li><p>“On Variational Bounds of Mutual Information” (PMLR 2019)</p></li>
<li><p>“Accurate Estimation of Mutual Information in High Dimensional Data” (ArXiv 2025)</p></li>
</ul>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code> focuses on two particularly effective estimators that cover the most common use cases.</p>
<section id="infonce-low-variance-high-bias">
<h3>2.1 InfoNCE (Low Variance, High Bias)<a class="headerlink" href="#infonce-low-variance-high-bias" title="Link to this heading"></a></h3>
<p>The <strong>InfoNCE</strong> (Noise-Contrastive Estimation) estimator is the workhorse of <code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code>. Its formula is:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) \ge \mathbb{E}\left[ f(x,y) - \log\left(\frac{1}{N}\sum_{j=1}^N e^{f(x,y_j)}\right) \right]\]</div>
<p><strong>Intuition:</strong> For each positive pair <cite>(x, y)</cite>, the critic <cite>f(x,y)</cite> tries to maximize its score relative to the scores of <cite>N-1</cite> negative pairs <cite>(x, y_j)</cite>. This is effectively a classification problem where the model tries to pick the “real” partner <cite>y</cite> for a given <cite>x</cite> out of a lineup of <cite>N</cite> candidates.</p>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p><strong>Low Variance:</strong> InfoNCE is known to be a very stable estimator, producing consistent results across different random seeds.</p></li>
<li><p><strong>Biased:</strong> It is a lower bound on the true MI. Crucially, this bound is <strong>theoretically limited by $log(N)$</strong>, where $N$ is the batch size. This means InfoNCE can never report an MI value higher than $log(N)$. For most applications where the true MI is modest, this is not a problem and its stability is a major advantage. This is why it’s the default in <code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code>.</p></li>
</ul>
</section>
<section id="smile-low-bias-moderate-variance">
<h3>2.2 SMILE (Low Bias, Moderate Variance)<a class="headerlink" href="#smile-low-bias-moderate-variance" title="Link to this heading"></a></h3>
<p>The <strong>SMILE</strong> (Smoothed Mutual Information “Lower-bound” Estimator) is designed to provide a less biased estimate, which is critical in scenarios where the true MI might be high.</p>
<div class="math notranslate nohighlight">
\[I(X;Y) \ge \mathbb{E}\left[ f(x,y) \right] - \log \mathbb{E}\left[ e^{\text{clip}(f(x,y'), \tau)} \right]\]</div>
<p><strong>Intuition:</strong> SMILE is similar to other classical estimators - like MINE <em>Mutual Information Neural Estimator</em> -, but it introduces a clipping function on the normalization factor. By clipping the scores at a value $tau$, it prevents a few “easy” samples from dominating the loss function, which is a major source of bias.</p>
<p><strong>Properties:</strong></p>
<ul>
<li><p><strong>Low Bias:</strong> By mitigating the impact of easy negatives, SMILE can provide estimates that are much closer to the true MI, especially when the MI is high. It is not strictly bounded by $log(N)$ in the same way as InfoNCE.</p></li>
<li><p><strong>Moderate Variance:</strong> This reduction in bias comes at the cost of slightly higher variance compared to InfoNCE.</p></li>
<li><p><strong>The ``clip`` parameter (``τ``):</strong> A clipping value of <code class="docutils literal notranslate"><span class="pre">τ=5</span></code> is often a robust default choice.</p>
<p><strong>Recommendation:</strong> Use InfoNCE for general-purpose, stable MI estimation. Use SMILE for tasks like dimensionality estimation where the true MI may be very high and you need a less biased estimator.</p>
</li>
</ul>
</section>
</section>
<section id="the-variational-approach">
<h2>3. The Variational Approach<a class="headerlink" href="#the-variational-approach" title="Link to this heading"></a></h2>
<p>Standard neural estimators learn a single embedding vector, $z = g(x)$, for each input. A variational approach, in contrast, learns a posterior distribution over the embeddings, $q(z|x)$. This is typically a Gaussian distribution parameterized by a mean and a variance vector, $(mu_x, sigma_x) = g(x)$.</p>
<p>The total loss function is modified to include a KL divergence term that acts as a regularizer, encouraging the learned posterior distributions to be close to a prior (usually a standard normal distribution):</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = (D_{KL}(p(z_x|x)||q(z_x)) + D_{KL}(p(z_y|y)||p(z_y))) - \beta  \hat{I_\text{estimator}}(Z_X;Z_Y)\]</div>
<p>This regularization can improve the quality of the learned representations and lead to more stable and robust MI estimates, particularly in complex, high-dimensional settings.</p>
</section>
<section id="the-problem-of-finite-sampling-bias">
<h2>4. The Problem of Finite-Sampling Bias<a class="headerlink" href="#the-problem-of-finite-sampling-bias" title="Link to this heading"></a></h2>
<p>Even with a perfect estimator, any analysis performed on a finite dataset of $N$ samples will be biased. The model will inevitably find spurious correlations in the random noise of the data, leading to a <strong>systematic overestimation</strong> of the true MI.</p>
<p>Theoretically, for a large number of samples $N$, this bias has a clear relationship with the sample size:</p>
<div class="math notranslate nohighlight">
\[I_{\text{estimated}}(N) \approx I_{\text{true}} + \frac{a}{N} + O\left(\frac{1}{N^2}\right)\]</div>
<p>This means the estimated MI is approximately linear in $1/N$. This is the key insight that <code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code> uses to correct for the bias.</p>
</section>
<section id="the-solution-rigorous-bias-correction">
<h2>5. The Solution: Rigorous Bias Correction<a class="headerlink" href="#the-solution-rigorous-bias-correction" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">mode='rigorous'</span></code> in <code class="docutils literal notranslate"><span class="pre">NeuralMI</span></code> automates a principled, multi-step workflow based on this theoretical relationship:</p>
<ol class="arabic simple">
<li><p><strong>Subsampling:</strong> The library repeatedly runs the MI estimation on different fractions of the data. For example, it might split the data into $gamma=2$ halves, then $gamma=3$ thirds, and so on.</p></li>
<li><p><strong>Fitting:</strong> It calculates the mean MI estimate for each data fraction size ($1/N$). Because the bias is linear in $1/N$, it fits a weighted linear regression to these points.</p></li>
<li><p><strong>Extrapolation:</strong> It extrapolates this line back to the y-intercept, which corresponds to $1/N = 0$—an infinite dataset. This intercept is the final, bias-corrected MI estimate. The confidence interval of this intercept provides the error bars.</p></li>
</ol>
<p>This procedure effectively subtracts the bias that is dependent on sample size, yielding a more accurate and scientifically rigorous result.</p>
<blockquote>
<div><p><strong>References:</strong></p>
<ul class="simple">
<li><p>“Estimation of mutual information for real-valued data with error bars and controlled bias” (PRE 2019)</p></li>
<li><p>“Accurate Estimation of Mutual Information in High Dimensional Data” (ArXiv 2025)</p></li>
</ul>
</div></blockquote>
</section>
<section id="estimating-latent-dimensionality">
<h2>6. Estimating Latent Dimensionality<a class="headerlink" href="#estimating-latent-dimensionality" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">mode='dimensionality'</span></code> uses a clever trick to estimate the complexity of a single neural population <code class="docutils literal notranslate"><span class="pre">X</span></code>. It randomly splits the channels of <code class="docutils literal notranslate"><span class="pre">X</span></code> into two halves, <code class="docutils literal notranslate"><span class="pre">X_A</span></code> and <code class="docutils literal notranslate"><span class="pre">X_B</span></code>, and measures the “Internal Information” <code class="docutils literal notranslate"><span class="pre">I(X_A;</span> <span class="pre">X_B)</span></code>.</p>
<p><strong>Intuition:</strong>
If both <code class="docutils literal notranslate"><span class="pre">X_A</span></code> and <code class="docutils literal notranslate"><span class="pre">X_B</span></code> are just different observations of the same underlying low-dimensional latent signal <code class="docutils literal notranslate"><span class="pre">Z</span></code>, then in theory, <code class="docutils literal notranslate"><span class="pre">I(X_A;</span> <span class="pre">X_B)</span> <span class="pre">=</span> <span class="pre">I(Z;</span> <span class="pre">Z)</span> <span class="pre">=</span> <span class="pre">\infty</span></code>. However, the <em>discoverable</em> information is constrained by the dimensionality of <code class="docutils literal notranslate"><span class="pre">Z</span></code>.</p>
<p>We can find this constraint by using a <strong>``SeparableCritic``</strong> and varying its <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code>. The <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> acts as a bottleneck.</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> &lt; <code class="docutils literal notranslate"><span class="pre">dim(Z)</span></code>, the model can’t capture all the shared information, and the estimated MI will be low.</p></li>
<li><p>As <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> approaches <code class="docutils literal notranslate"><span class="pre">dim(Z)</span></code>, the estimated MI will rise.</p></li>
<li><p>Once <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">dim(Z)</span></code>, the model has enough capacity, and the MI will <strong>saturate</strong>. The point of saturation is our estimate for the latent dimensionality of <code class="docutils literal notranslate"><span class="pre">Z</span></code>.</p></li>
</ul>
<p>For this specific task, <strong>SMILE is often a better estimator than InfoNCE</strong>. Because the true MI can be very high, InfoNCE might saturate at its theoretical limit of $log(N)$ <em>before</em> the model’s capacity (<code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code>) becomes the true bottleneck. SMILE’s lower bias allows the curve to rise higher, revealing the true saturation point more clearly.</p>
<p>Note that this process can also be done for regular <code class="docutils literal notranslate"><span class="pre">I(X;Y)</span></code>, informing us about the intrinsic dimensionality of the <em>interaction</em> space. Here, the information won’t be theoretically infinite, and InfoNCE probably will be better.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="further_reading.html" class="btn btn-neutral float-left" title="Further Reading" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="UNDER_THE_HOOD.html" class="btn btn-neutral float-right" title="Under the Hood: How NeuralMI Works" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Eslam Abdelaleem.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>